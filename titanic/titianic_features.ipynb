{
 "metadata": {
  "name": "titianic_features"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For simplicity, we are going to extract all variables as discrete ones. So we will be binning continuous variables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import csv\n",
      "\n",
      "def bin_fare(fare):\n",
      "    \"\"\"\n",
      "        1 = under 25 (default)\n",
      "        2 = 25-70\n",
      "        3 = over 70\n",
      "    \"\"\"\n",
      "    if not fare:\n",
      "        return 1\n",
      "    fare = float(fare)\n",
      "    if 70 <= fare:\n",
      "        return 3\n",
      "    if 25 <= fare <= 70:\n",
      "        return 2\n",
      "    return 1\n",
      "\n",
      "def bin_age(age):\n",
      "    \"\"\" \n",
      "        1 = under 18\n",
      "        2 = 18-35 (default)\n",
      "        3 = 35-55\n",
      "        4 = over 55\n",
      "    \"\"\"\n",
      "    if not age:\n",
      "        return 2\n",
      "    age = float(age)\n",
      "    if age <= 18:\n",
      "        return 1\n",
      "    if 35 <= age <= 55:\n",
      "        return 3\n",
      "    if 55 <= age: \n",
      "        return 4\n",
      "    return 2\n",
      "    \n",
      "\n",
      "def load_data(path, is_training):\n",
      "    \"\"\"\n",
      "    is_training = \n",
      "        True:  Return raw Training Set and Labels\n",
      "        False: Return raw Test Set\n",
      "    \"\"\"\n",
      "    with open(path) as csvfile:\n",
      "        training_file = csv.DictReader(csvfile)\n",
      "        data = []\n",
      "        labels = []\n",
      "        for row in training_file:\n",
      "            if is_training:\n",
      "                labels.append(row['survived'])\n",
      "                del row['survived'] # we can't train on survived!\n",
      "            del row['name'] # for simplicity, we won't try to analyze name, ticket, or cabin\n",
      "            del row['ticket']\n",
      "            del row['cabin']\n",
      "            row['fare'] = bin_fare(row['fare'])\n",
      "            row['age'] = bin_age(row['age'])\n",
      "            row['sibsp'] = int(row['sibsp'])\n",
      "            row['parch'] = int(row['parch'])\n",
      "            row['pclass'] = int(row['pclass'])\n",
      "            row['embarked'] = row['embarked'] if row['embarked'] else 'S'\n",
      "            data.append(row)\n",
      "        if is_training:    \n",
      "            return np.array(data), np.array(labels)\n",
      "        else:\n",
      "            return np.array(data)\n",
      "\n",
      "training_data, labels = load_data('data/train.csv', True)    \n",
      "print training_data[0], labels[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'fare': 1, 'embarked': 'S', 'age': 2, 'parch': 0, 'pclass': 3, 'sex': 'male', 'sibsp': 1} 0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections\n",
      "import itertools\n",
      "\n",
      "columns = training_data[0].keys()\n",
      "\n",
      "# {column: [possible values], ...}\n",
      "column_bins = { column: collections.Counter(row[column] for row in training_data).keys() for column in columns }\n",
      "\n",
      "# [(column1, column2), (column1, column3), ...]\n",
      "combined_columns = [pair for pair in itertools.combinations(column_bins.keys(), 2)]\n",
      "\n",
      "extended_training_data = []\n",
      "for row in training_data:\n",
      "    extension = {\"{}_{}\".format(pair[0], pair[1]) : \"{}_{}\".format(row[pair[0]], row[pair[1]]) for pair in combined_columns}\n",
      "    extended_training_data.append(dict(row, **extension))\n",
      "\n",
      "extended_training_data = np.array(extended_training_data)\n",
      "print extended_training_data[-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{'fare': 1, 'fare_sex': '1_male', 'sex_sibsp': 'male_0', 'fare_parch': '1_0', 'fare_sibsp': '1_0', 'embarked_pclass': 'Q_3', 'embarked_sex': 'Q_male', 'parch_pclass': '0_3', 'age_parch': '2_0', 'sex': 'male', 'parch_sex': '0_male', 'age_pclass': '2_3', 'pclass_sex': '3_male', 'sibsp': 0, 'embarked_age': 'Q_2', 'fare_age': '1_2', 'fare_embarked': '1_Q', 'age_sibsp': '2_0', 'pclass_sibsp': '3_0', 'fare_pclass': '1_3', 'parch_sibsp': '0_0', 'embarked': 'Q', 'embarked_parch': 'Q_0', 'age': 2, 'parch': 0, 'pclass': 3, 'embarked_sibsp': 'Q_0', 'age_sex': '2_male'}\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%pylab inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Welcome to pylab, a matplotlib-based Python environment [backend: module://IPython.zmq.pylab.backend_inline].\n",
        "For more information, type 'help(pylab)'.\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import StringIO\n",
      "\n",
      "import pydot\n",
      "from IPython.core.display import Image\n",
      "from sklearn.feature_extraction import DictVectorizer\n",
      "from sklearn import tree\n",
      "from sklearn import cross_validation\n",
      "\n",
      "def dtree(training_data, labels):\n",
      "    '''Creates a decision tree from np.array of dicts'''\n",
      "    vectorizer = DictVectorizer()\n",
      "    vectorizer.fit(training_data)\n",
      "    cleaned_training_data = vectorizer.transform(training_data).toarray()\n",
      "    \n",
      "    classifier = tree.DecisionTreeClassifier(min_samples_leaf=5, max_depth=5)\n",
      "    classifier.fit(cleaned_training_data, labels)\n",
      "    \n",
      "    scores = cross_validation.cross_val_score(classifier, cleaned_training_data, labels, cv=5)\n",
      "    print \"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() / 2)\n",
      "    return classifier, vectorizer\n",
      "\n",
      "def graph_dtree(classifier, vectorizer):\n",
      "    \n",
      "    dot_data = StringIO.StringIO() \n",
      "    tree.export_graphviz(classifier, out_file=dot_data, feature_names=vectorizer.get_feature_names()) \n",
      "    graph = pydot.graph_from_dot_data(dot_data.getvalue()) \n",
      "    graph.write_png(\"decision_tree.png\") \n",
      "    print \"Saved to decision_tree.png\" \n",
      "\n",
      "def display_graph(filename='decision_tree.png'):\n",
      "    return Image(filename=filename)\n",
      "    \n",
      "clf, vec = dtree(extended_training_data, labels)\n",
      "graph_dtree(clf, vec)\n",
      "#display_graph()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Accuracy: 0.80 (+/- 0.02)\n",
        "Saved to decision_tree.png"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data = load_data('data/test.csv', False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Experimentation Area ###"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "d = {1:11}\n",
      "d2 = dict(d, **{2:22})\n",
      "print d\n",
      "print d2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{1: 11}\n",
        "{1: 11, 2: 22}\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Building a function to measure the amount of information in a given sample. \n",
      "I made this up, so it's not necessarily the best approach.\n",
      "'''\n",
      "\n",
      "import math\n",
      "def correlation(labels):\n",
      "    total = len(labels)\n",
      "    lived = sum(int(l) for l in labels)\n",
      "    died = total-lived\n",
      "    try:\n",
      "        return (abs(lived-died)/float(total))\n",
      "    except ZeroDivisionError:\n",
      "        return -1\n",
      "\n",
      "def weighted_correlation(labels):\n",
      "    try:\n",
      "        return correlation(labels) * math.log(len(labels))\n",
      "    except ValueError:\n",
      "        return -1\n",
      "\n",
      "#print correlation(labels)\n",
      "\n",
      "l1 = [0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,1,0,0,0,0,0,0,0,0,0,0,]\n",
      "l2 = [1,1,1,1,1,1,0]\n",
      "print correlation(l1), weighted_correlation(l1)\n",
      "print\n",
      "print correlation(l2), weighted_correlation(l2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Testing out the correlation to see which combinations of features give the most information.\n",
      "'''\n",
      "\n",
      "columns = training_data[0].keys()\n",
      "\n",
      "import collections\n",
      "column_bins = { column: collections.Counter(row[column] for row in training_data).keys() for column in columns }\n",
      "\n",
      "def select_labels(data, key_1, value_1, key_2, value_2):\n",
      "    result = []\n",
      "    for i, row in enumerate(data):\n",
      "        if row[key_1] == value_1 and row[key_2] == value_2:\n",
      "            result += labels[i] \n",
      "    return result\n",
      "\n",
      "import itertools\n",
      "second_order_corr = {}\n",
      "for pair in itertools.combinations(column_bins.keys(), 2):\n",
      "    for inner_pair in itertools.product(column_bins[pair[0]], column_bins[pair[1]]):\n",
      "        second_order_corr[\"{}={}, {}={}\".format(pair[0], inner_pair[0], pair[1], inner_pair[1])] = \\\n",
      "            weighted_correlation(select_labels(training_data, pair[0], inner_pair[0], pair[1], inner_pair[1]))\n",
      "    \n",
      "#>>> # dictionary sorted by value\n",
      "#>>> OrderedDict(sorted(d.items(), key=lambda t: t[1]))\n",
      "        \n",
      "second_order_corr = collections.OrderedDict(sorted(second_order_corr.items(), key=lambda t: t[1], reverse=True))\n",
      "    \n",
      "for k, v in second_order_corr.iteritems():\n",
      "    print k, v\n",
      "\n",
      "#print correlation(select_labels(training_data, 'embarked', 'Q', 'age', 1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}